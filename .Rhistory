overfitting_check <- train_metrics %>%
dplyr::select(model, MAE_train = MAE, RMSE_train = RMSE, corr_train = correlation) %>%
left_join(
test_metrics %>% dplyr::select(model, MAE_test = MAE, RMSE_test = RMSE, corr_test = correlation),
by = "model"
) %>%
mutate(
MAE_degradation = (MAE_test - MAE_train) / MAE_train * 100,
RMSE_degradation = (RMSE_test - RMSE_train) / RMSE_train * 100,
corr_degradation = (corr_train - corr_test) / corr_train * 100
)
overfitting_check %>%
kable(digits = 2, caption = "Train vs Test Performance: Overfitting Assessment") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
cat("OVERFITTING INTERPRETATION:\n")
cat("• MAE/RMSE degradation < 20%: Good generalization\n")
cat("• MAE/RMSE degradation 20-50%: Moderate overfitting\n")
cat("• MAE/RMSE degradation > 50%: Severe overfitting\n")
# Assess overall risk.
worst_mae_degrad <- max(overfitting_check$MAE_degradation)
cat(sprintf("\nWorst MAE degradation across models: %.1f%%\n", worst_mae_degrad))
if (worst_mae_degrad < 20) {
cat("ASSESSMENT: Good generalization - models not overfitting.\n")
} else if (worst_mae_degrad < 50) {
cat("ASSESSMENT: Moderate overfitting detected - consider simpler models.\n")
} else {
cat("ASSESSMENT: Severe overfitting - model complexity should be reduced.\n")
}
# Compare train vs test performance
overfitting_check <- train_metrics %>%
dplyr::select(model, MAE_train = MAE, RMSE_train = RMSE, corr_train = correlation) %>%
left_join(
test_metrics %>% dplyr::select(model, MAE_test = MAE, RMSE_test = RMSE, corr_test = correlation),
by = "model"
) %>%
mutate(
MAE_degradation = (MAE_test - MAE_train) / MAE_train * 100,
RMSE_degradation = (RMSE_test - RMSE_train) / RMSE_train * 100,
corr_degradation = (corr_train - corr_test) / corr_train * 100
)
overfitting_check %>%
kable(digits = 2, caption = "Train vs Test Performance: Overfitting Assessment") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Use Model 3 for downstream analysis, this is for operational deployment.
model_nb_enhanced <- model_nb_3
df_test_pred <- test_pred_3
df_train_pred <- train_pred_3
# Extract scalar metrics for the selected model.
selected_test_metrics <- test_metrics_3
cat("SELECTED MODEL: Model 3 (+ ACS)\n")
cat(sprintf("Test MAE: %.3f\n", selected_test_metrics$MAE))
cat(sprintf("Test RMSE: %.3f\n", selected_test_metrics$RMSE))
cat(sprintf("Test Correlation: %.3f\n", selected_test_metrics$Correlation))
# Use Model 3 for downstream analysis, this is for operational deployment.
model_nb_enhanced <- model_nb_3
df_test_pred <- test_pred_3
df_train_pred <- train_pred_3
# Extract scalar metrics for the selected model.
selected_test_metrics <- test_metrics_3
cat("SELECTED MODEL: Model 3 (+ ACS)\n")
cat(sprintf("Test MAE: %.3f\n", selected_test_metrics$MAE))
cat(sprintf("Test RMSE: %.3f\n", selected_test_metrics$RMSE))
cat(sprintf("Test Correlation: %.3f\n", selected_test_metrics$correlation))
# Use Model 3 for downstream analysis, this is for operational deployment.
model_nb_enhanced <- model_nb_4
df_test_pred <- test_pred_4
df_train_pred <- train_pred_4
# Extract scalar metrics for the selected model.
selected_test_metrics <- test_metrics_4
cat("SELECTED MODEL: Model 3 (+ ACS)\n")
cat(sprintf("Test MAE: %.3f\n", selected_test_metrics$MAE))
cat(sprintf("Test RMSE: %.3f\n", selected_test_metrics$RMSE))
cat(sprintf("Test Correlation: %.3f\n", selected_test_metrics$correlation))
# Use Model 3 for downstream analysis, this is for operational deployment.
model_nb_enhanced <- model_nb_3
df_test_pred <- test_pred_3
df_train_pred <- train_pred_3
# Extract scalar metrics for the selected model.
selected_test_metrics <- test_metrics_3
cat("SELECTED MODEL: Model 3 (+ ACS)\n")
cat(sprintf("Test MAE: %.3f\n", selected_test_metrics$MAE))
cat(sprintf("Test RMSE: %.3f\n", selected_test_metrics$RMSE))
cat(sprintf("Test Correlation: %.3f\n", selected_test_metrics$correlation))
#| fig-dpi: 300
#| fig-height: 10
#| fig-width: 10
# Create scatter plot of predicted vs observed values using selected model.
pred_obs_plot <- df_test_pred %>%
ggplot(aes(x = predicted, y = filings_count)) +
geom_hex(bins = 40) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "#E74C3C", linewidth = 1) +
scale_fill_viridis_c(option = "plasma", trans = "log10", labels = comma) +
scale_x_continuous(limits = c(0, 20)) +
scale_y_continuous(limits = c(0, 20)) +
labs(
title = "Predicted vs Observed Eviction Filings (Test Set)",
subtitle = sprintf("MAE = %.2f | RMSE = %.2f | Correlation = %.3f",
selected_test_metrics$MAE,
selected_test_metrics$RMSE,
selected_test_metrics$Correlation),
x = "Predicted Filings",
y = "Observed Filings",
fill = "Count",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set | Model 4"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "right"
)
# Display predicted vs observed plot.
pred_obs_plot
#| fig-dpi: 300
#| fig-height: 10
#| fig-width: 10
# Create scatter plot of predicted vs observed values using selected model.
pred_obs_plot <- df_test_pred %>%
ggplot(aes(x = predicted, y = filings_count)) +
geom_hex(bins = 40) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "#E74C3C", linewidth = 1) +
scale_fill_viridis_c(option = "plasma", trans = "log10", labels = comma) +
scale_x_continuous(limits = c(0, 20)) +
scale_y_continuous(limits = c(0, 20)) +
labs(
title = "Predicted vs Observed Eviction Filings (Test Set)",
subtitle = sprintf("MAE = %.2f | RMSE = %.2f | Correlation = %.3f",
selected_test_metrics$MAE,
selected_test_metrics$RMSE,
selected_test_metrics$correlation),
x = "Predicted Filings",
y = "Observed Filings",
fill = "Count",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set | Model 4"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "right"
)
# Display predicted vs observed plot.
pred_obs_plot
# Calculate test set metrics by racial majority.
test_metrics_by_race <- df_test_pred %>%
filter(!is.na(racial_majority), racial_majority != "") %>%
group_by(racial_majority) %>%
summarize(
n = n(),
mae = mean(abs_error, na.rm = TRUE),
rmse = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
mean_error = mean(residual, na.rm = TRUE),
.groups = "drop"
)
test_metrics_by_race %>%
kable(digits = 2, caption = "Test Set Performance by Tract Racial Majority") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
#| fig-dpi: 300
#| fig-height: 6
#| fig-width: 12
# Create residual distribution plot.
residual_plot <- df_test_pred %>%
ggplot(aes(x = residual)) +
geom_histogram(bins = 50, fill = "#3498DB", color = "white", alpha = 0.8) +
geom_vline(xintercept = 0, linetype = "dashed", color = "#E74C3C", linewidth = 1) +
scale_x_continuous(limits = c(-15, 15)) +
labs(
title = "Distribution of Prediction Residuals (Test Set)",
subtitle = "Centered near zero indicates unbiased predictions",
x = "Residual (Observed - Predicted)",
y = "Frequency",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set"
) +
theme_minimal(base_size = 12) +
theme(plot.title = element_text(face = "bold", size = 14))
# Display residual plot.
residual_plot
# Create risk quintile classification based on predicted values.
df_test_risk <- df_test_pred %>%
mutate(
risk_quintile = ntile(predicted, 5),
risk_category = case_when(
risk_quintile == 1 ~ "Lowest Risk",
risk_quintile == 2 ~ "Low Risk",
risk_quintile == 3 ~ "Moderate Risk",
risk_quintile == 4 ~ "High Risk",
risk_quintile == 5 ~ "Highest Risk"
),
risk_category = factor(risk_category,
levels = c("Lowest Risk", "Low Risk", "Moderate Risk",
"High Risk", "Highest Risk"))
)
# Calculate observed filings by risk category.
risk_validation <- df_test_risk %>%
group_by(risk_category) %>%
summarize(
n_tract_months = n(),
mean_observed = mean(filings_count, na.rm = TRUE),
median_observed = median(filings_count, na.rm = TRUE),
total_filings = sum(filings_count, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(pct_total_filings = total_filings / sum(total_filings) * 100)
# Display risk validation table.
risk_validation %>%
kable(digits = 2, caption = "Observed Filings by Predicted Risk Category") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
#| fig-dpi: 300
#| fig-height: 6
#| fig-width: 10
# Create bar plot showing observed filings by risk category.
risk_bar_plot <- risk_validation %>%
ggplot(aes(x = risk_category, y = mean_observed, fill = risk_category)) +
geom_col(width = 0.7) +
geom_text(aes(label = sprintf("%.1f%%", pct_total_filings)),
vjust = -0.5, size = 4) +
scale_fill_viridis_d(option = "plasma", direction = -1) +
scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
labs(
title = "Mean Observed Filings by Predicted Risk Category",
subtitle = "Higher predicted risk categories correctly capture higher observed filing rates",
x = "Predicted Risk Category",
y = "Mean Observed Filings per Tract-Month",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
axis.text.x = element_text(angle = 0, hjust = 0.5),
legend.position = "none"
)
# Display risk bar plot.
risk_bar_plot
# Profile characteristics of highest risk tract-months.
high_risk_profile <- df_test_risk %>%
filter(risk_category == "Highest Risk") %>%
group_by(racial_majority) %>%
summarize(
n_tract_months = n(),
pct_of_high_risk = n() / nrow(df_test_risk %>% filter(risk_category == "Highest Risk")) * 100,
mean_filings = mean(filings_count, na.rm = TRUE),
.groups = "drop"
) %>%
arrange(desc(n_tract_months))
high_risk_profile %>%
kable(digits = 1, caption = "Racial Composition of Highest Risk Tract-Months") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
#| fig-dpi: 300
#| fig-height: 10
#| fig-width: 10
# Extract standardized coefficients for interpretation.
feature_importance <- broom::tidy(model_nb_enhanced) %>%
filter(term != "(Intercept)") %>%
mutate(
abs_estimate = abs(estimate),
direction = ifelse(estimate > 0, "Positive", "Negative"),
# Clean term names for display.
term_clean = str_replace(term, "factor\\(month_num\\)", "Month ") %>%
str_replace("factor\\(racial_majority\\)", "") %>%
str_replace("filings_", "Lag: ") %>%
str_replace("spatial_lag_filings", "Spatial Lag (Lagged)") %>%
str_replace("log_avg_balance", "Tax Delinquency (Log)") %>%
str_replace("pct_renter", "ACS: % Renter") %>%
str_replace("poverty_rate", "ACS: Poverty Rate") %>%
str_replace("severe_rent_burden", "ACS: Severe Rent Burden") %>%
str_replace("unemployment_rate", "ACS: Unemployment Rate") %>%
str_replace("log_median_income", "ACS: Median Income (Log)") %>%
str_replace("pct_single_mother", "ACS: % Single Mother HH"),
# Identify feature category.
category = case_when(
str_detect(term_clean, "Lag") ~ "Temporal",
str_detect(term_clean, "Spatial") ~ "Spatial",
str_detect(term_clean, "ACS") ~ "Socioeconomic (ACS)",
str_detect(term_clean, "Tax") ~ "Tax Delinquency",
str_detect(term_clean, "Month") ~ "Seasonality",
str_detect(term_clean, "Moratorium") ~ "Policy",
TRUE ~ "Demographic"
)
) %>%
arrange(desc(abs_estimate)) %>%
head(20)
# Create feature importance plot.
importance_plot <- feature_importance %>%
ggplot(aes(x = reorder(term_clean, abs_estimate), y = estimate, fill = category)) +
geom_col(width = 0.7) +
coord_flip() +
scale_fill_brewer(palette = "Set2") +
labs(
title = "Top 20 Most Important Predictors",
subtitle = "Coefficient magnitude indicates effect size on log(expected filings)",
x = "Predictor Variable",
y = "Coefficient Estimate",
fill = "Category",
caption = "Enhanced Negative Binomial Regression Coefficients (including ACS indicators)"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "bottom"
)
# Display importance plot.
importance_plot
# Specifically analyze contribution of ACS features.
acs_contribution <- broom::tidy(model_nb_enhanced) %>%
filter(str_detect(term, "pct_renter|poverty_rate|severe_rent_burden|unemployment_rate")) %>%
mutate(
irr = exp(estimate),
pct_change = (irr - 1) * 100,
significant = ifelse(p.value < 0.05, "Yes", "No")
) %>%
dplyr::select(term, estimate, std.error, p.value, irr, pct_change, significant)
cat("ACS FEATURE CONTRIBUTION SUMMARY\n")
cat("These socioeconomic indicators capture structural vulnerability.\n\n")
acs_contribution %>%
kable(digits = 3, caption = "ACS Socioeconomic Feature Effects") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Calculate incidence rate ratios for interpretation.
irr_table <- broom::tidy(model_nb_enhanced) %>%
mutate(
irr = exp(estimate),
irr_lower = exp(estimate - 1.96 * std.error),
irr_upper = exp(estimate + 1.96 * std.error),
pct_change = (irr - 1) * 100
) %>%
filter(term != "(Intercept)") %>%
dplyr::select(term, irr, irr_lower, irr_upper, pct_change, p.value)
irr_table %>%
head(15) %>%
kable(digits = 3, caption = "Incidence Rate Ratios (IRR) for Key Predictors") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Create actionable output for city agencies with intervention thresholds.
operational_output <- df_test_pred %>%
filter(predicted >= 5) %>%
group_by(GEOID) %>%
summarize(
predicted_filings_next_month = mean(predicted),
n_high_risk_months = n(),
.groups = "drop"
) %>%
left_join(df_acs, by = "GEOID") %>%
arrange(desc(predicted_filings_next_month))
# Create actionable output for city agencies with intervention thresholds.
operational_output <- df_test_pred %>%
filter(predicted >= 5) %>%
group_by(GEOID) %>%
summarize(
predicted_filings_next_month = mean(predicted),
n_high_risk_months = n(),
.groups = "drop"
) %>%
left_join(df_acs, by = "GEOID") %>%
arrange(desc(predicted_filings_next_month))
operational_output
#| fig-dpi: 300
#| fig-height: 7
#| fig-width: 12
# Calculate prediction errors by racial majority to assess disparate impact.
equity_metrics <- df_test_pred %>%
filter(!is.na(racial_majority), racial_majority != "") %>%
group_by(racial_majority) %>%
summarize(
n_obs = n(),
mean_observed = mean(filings_count, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
mae = mean(abs_error, na.rm = TRUE),
mean_error = mean(residual, na.rm = TRUE),
under_prediction_rate = sum(residual > 0) / n() * 100,
.groups = "drop"
)
# Display equity metrics table.
equity_metrics %>%
kable(digits = 2, caption = "Model Performance Metrics by Tract Racial Majority") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
#| fig-dpi: 300
#| fig-height: 6
#| fig-width: 12
# Create plot comparing MAE across racial categories.
equity_plot <- equity_metrics %>%
ggplot(aes(x = racial_majority, y = mae, fill = racial_majority)) +
geom_col(width = 0.7) +
geom_text(aes(label = sprintf("%.2f", mae)), vjust = -0.5, size = 4) +
scale_fill_manual(values = racial_colors) +
scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +
labs(
title = "Model Accuracy (MAE) by Tract Racial Majority",
subtitle = "Lower values indicate better prediction accuracy",
x = "Tract Racial Majority",
y = "Mean Absolute Error",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "none"
)
# Display equity plot.
equity_plot
#| fig-dpi: 300
#| fig-height: 8
#| fig-width: 12
# Create calibration plot showing predicted vs observed by racial majority.
calibration_plot <- df_test_pred %>%
filter(!is.na(racial_majority), racial_majority != "") %>%
mutate(predicted_bin = cut(predicted, breaks = seq(0, 15, by = 1), include.lowest = TRUE)) %>%
filter(!is.na(predicted_bin)) %>%
group_by(racial_majority, predicted_bin) %>%
summarize(
mean_predicted = mean(predicted, na.rm = TRUE),
mean_observed = mean(filings_count, na.rm = TRUE),
n = n(),
.groups = "drop"
) %>%
filter(n >= 10) %>%
ggplot(aes(x = mean_predicted, y = mean_observed, color = racial_majority)) +
geom_point(aes(size = n), alpha = 0.7) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
scale_color_manual(values = racial_colors) +
scale_size_continuous(range = c(2, 8)) +
labs(
title = "Model Calibration by Tract Racial Majority",
subtitle = "Points on diagonal indicate well-calibrated predictions",
x = "Mean Predicted Filings",
y = "Mean Observed Filings",
color = "Racial\nMajority",
size = "N Obs",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "right"
)
# Display calibration plot.
calibration_plot
#| fig-dpi: 300
#| fig-height: 7
#| fig-width: 12
# Calculate risk category distribution by racial majority.
risk_by_race <- df_test_risk %>%
filter(!is.na(racial_majority), racial_majority != "") %>%
group_by(racial_majority, risk_category) %>%
summarize(n = n(), .groups = "drop") %>%
group_by(racial_majority) %>%
mutate(pct = n / sum(n) * 100) %>%
ungroup()
# Create stacked bar plot.
risk_race_plot <- risk_by_race %>%
ggplot(aes(x = racial_majority, y = pct, fill = risk_category)) +
geom_col(width = 0.7) +
scale_fill_viridis_d(option = "plasma", direction = -1) +
scale_y_continuous(labels = percent_format(scale = 1)) +
labs(
title = "Distribution of Risk Categories by Tract Racial Majority",
subtitle = "Black-majority tracts disproportionately classified as high risk",
x = "Tract Racial Majority",
y = "Percentage of Tract-Months",
fill = "Risk\nCategory",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "right"
)
# Display risk by race plot.
risk_race_plot
#| fig-dpi: 300
#| fig-height: 6
#| fig-width: 12
# Create equity comparison plot.
equity_plot <- equity_metrics %>%
pivot_longer(cols = c(mean_observed, mean_predicted),
names_to = "type", values_to = "value") %>%
mutate(type = ifelse(type == "mean_observed", "observed", "predicted")) %>%
ggplot(aes(x = racial_majority, y = value, fill = type)) +
geom_col(position = "dodge", width = 0.7) +
scale_fill_manual(values = c("observed" = "#E74C3C", "predicted" = "#3498DB")) +
labs(
title = "Model Calibration by Racial Majority: Observed vs Predicted",
subtitle = "Close alignment indicates equitable prediction accuracy across communities",
x = "Tract Racial Majority",
y = "Mean Filings per Tract-Month",
fill = "type",
caption = "Data: Philadelphia Eviction Filings 2024-2025 Test Set"
) +
theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14),
legend.position = "bottom"
)
# Display equity plot.
equity_plot
# Compile comprehensive summary statistics.
summary_stats <- df_monthly %>%
filter(GEOID != "sealed", !is.na(GEOID), GEOID != "") %>%
summarize(
total_obs = n(),
unique_tracts = n_distinct(GEOID),
unique_months = n_distinct(date),
mean_filings = mean(filings_count, na.rm = TRUE),
median_filings = median(filings_count, na.rm = TRUE),
sd_filings = sd(filings_count, na.rm = TRUE),
min_filings = min(filings_count, na.rm = TRUE),
max_filings = max(filings_count, na.rm = TRUE),
zero_pct = sum(filings_count == 0) / n() * 100,
dispersion = var(filings_count) / mean(filings_count)
)
summary_stats %>%
pivot_longer(everything(), names_to = "statistic", values_to = "value") %>%
mutate(value = round(value, 2)) %>%
kable(caption = "Comprehensive Summary Statistics") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
