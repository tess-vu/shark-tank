# Plot 3: Predicted vs Observed (RAW - to see if this is the problem)
p3 <- train_pred_1 %>%
ggplot(aes(x = predicted, y = filings_count)) +
geom_point(alpha = 0.3, color = "#E74C3C") +
geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
labs(title = "Predicted vs Observed (RAW uncapped)",
subtitle = "If points go way above diagonal, this is the problem",
x = "Predicted",
y = "Observed (uncapped)") +
theme_minimal()
# Plot 4: Comparison
comparison_data <- train_pred_1 %>%
select(predicted, filings_count, filings_count_capped) %>%
pivot_longer(cols = c(filings_count, filings_count_capped),
names_to = "type", values_to = "observed") %>%
mutate(type = ifelse(type == "filings_count", "RAW (uncapped)", "CAPPED at 20"))
p4 <- ggplot(comparison_data, aes(x = predicted, y = observed, color = type)) +
geom_point(alpha = 0.3) +
geom_abline(slope = 1, intercept = 0, color = "black", linetype = "dashed") +
scale_color_manual(values = c("RAW (uncapped)" = "#E74C3C", "CAPPED at 20" = "#3498DB")) +
labs(title = "RAW vs CAPPED: Which one matches your residuals?",
subtitle = "Blue points should cluster near diagonal if using capped correctly",
x = "Predicted",
y = "Observed",
color = "Observed Type") +
theme_minimal() +
theme(legend.position = "bottom")
# Combine plots
(p1 | p2) / (p3 | p4) +
plot_annotation(
title = "Training RMSE Diagnostic Plots",
subtitle = "If Plot 3 shows extreme outliers far from diagonal, you're using RAW instead of CAPPED"
)
# One final smoking gun test
cat("\n=== FINAL VERDICT ===\n")
cat(sprintf("Your reported training RMSE: 473.77\n"))
cat(sprintf("RMSE if using CAPPED correctly: %.2f\n",
sqrt(mean((df_train$filings_count_capped - predict(model_nb_1, df_train, type = "response"))^2, na.rm = TRUE))))
cat(sprintf("RMSE if using RAW accidentally: %.2f\n",
sqrt(mean((df_train$filings_count - predict(model_nb_1, df_train, type = "response"))^2, na.rm = TRUE))))
# Check what train_metrics_1 actually contains
cat("\nWhat's in your train_metrics_1:\n")
print(train_metrics_1)
# Combine plots
(p1 | p2) / (p3 | p4) +
plot_annotation(
title = "Training RMSE Diagnostic Plots",
subtitle = "If Plot 3 shows extreme outliers far from diagonal, you're using RAW instead of CAPPED"
)
# Check if model coefficients are reasonable
cat("\nModel Coefficients:\n")
coef_summary <- summary(model_nb_1)$coefficients
print(coef_summary[1:10,])  # First 10 coefficients
# Look for extreme values
cat("\nExtreme coefficients (abs > 10):\n")
print(coef_summary[abs(coef_summary[,1]) > 10,])
# What are the predictions?
train_preds <- predict(model_nb_1, df_train, type = "response")
cat(sprintf("\nPrediction range:\n"))
cat(sprintf("  Min: %.2f\n", min(train_preds)))
cat(sprintf("  Max: %.2f\n", max(train_preds)))
cat(sprintf("  Mean: %.2f\n", mean(train_preds)))
cat(sprintf("  Median: %.2f\n", median(train_preds)))
cat(sprintf("  95th percentile: %.2f\n", quantile(train_preds, 0.95)))
cat(sprintf("  99th percentile: %.2f\n", quantile(train_preds, 0.99)))
cat(sprintf("  N > 100: %d\n", sum(train_preds > 100)))
cat(sprintf("  N > 1000: %d\n", sum(train_preds > 1000)))
print(worst_tract)
# Check the features for the worst prediction
worst_tract <- df_train %>%
filter(GEOID == "42101027200", date == "2022-09-01")
print(worst_tract)
# Fit baseline Negative Binomial model with core time, policy, and monthly predictors.
model_nb_1 <- glm.nb(filings_count_capped ~ filings_ma3 + moratorium_active + factor(month_num) + is_extreme_spike,
data = df_train)
# Display 1st model summary.
cat("BASELINE NEGATIVE BINOMIAL MODEL SUMMARY\n")
summary(model_nb_1)
# Check if model coefficients are reasonable
cat("\nModel Coefficients:\n")
coef_summary <- summary(model_nb_4)$coefficients
print(coef_summary[1:10,])  # First 10 coefficients
# Look for extreme values
cat("\nExtreme coefficients (abs > 10):\n")
print(coef_summary[abs(coef_summary[,1]) > 10,])
# What are the predictions?
train_preds <- predict(model_nb_1, df_train, type = "response")
cat(sprintf("\nPrediction range:\n"))
cat(sprintf("  Min: %.2f\n", min(train_preds)))
cat(sprintf("  Max: %.2f\n", max(train_preds)))
cat(sprintf("  Mean: %.2f\n", mean(train_preds)))
cat(sprintf("  Median: %.2f\n", median(train_preds)))
cat(sprintf("  95th percentile: %.2f\n", quantile(train_preds, 0.95)))
cat(sprintf("  99th percentile: %.2f\n", quantile(train_preds, 0.99)))
cat(sprintf("  N > 100: %d\n", sum(train_preds > 100)))
cat(sprintf("  N > 1000: %d\n", sum(train_preds > 1000)))
cat(sprintf("N > 50: %d\n", sum(df_train$filings_ma3 > 50, na.rm = TRUE)))
cat("\nfilings_ma3 statistics in training data:\n")
cat(sprintf("Min: %.2f\n", min(df_train$filings_ma3, na.rm = TRUE)))
cat(sprintf("Max: %.2f\n", max(df_train$filings_ma3, na.rm = TRUE)))
cat(sprintf("Mean: %.2f\n", mean(df_train$filings_ma3, na.rm = TRUE)))
cat(sprintf("N > 20: %d\n", sum(df_train$filings_ma3 > 20, na.rm = TRUE)))
cat(sprintf("N > 50: %d\n", sum(df_train$filings_ma3 > 50, na.rm = TRUE)))
# Cap the momentum signal
df_train <- df_train %>% mutate(filings_ma3 = pmin(filings_ma3, 20))
df_test <- df_test %>% mutate(filings_ma3 = pmin(filings_ma3, 20))
cat("\nfilings_ma3 statistics in training data:\n")
cat(sprintf("Min: %.2f\n", min(df_train$filings_ma3, na.rm = TRUE)))
cat(sprintf("Max: %.2f\n", max(df_train$filings_ma3, na.rm = TRUE)))
cat(sprintf("Mean: %.2f\n", mean(df_train$filings_ma3, na.rm = TRUE)))
cat(sprintf("N > 20: %d\n", sum(df_train$filings_ma3 > 20, na.rm = TRUE)))
cat(sprintf("N > 50: %d\n", sum(df_train$filings_ma3 > 50, na.rm = TRUE)))
# Define temporal cutoff for train/test split.
train_end_date <- as.Date("2023-12-31")
test_start_date <- as.Date("2024-01-01")
# Filter for all variables needed by most complex model.
df_filtered <- df_model_prep %>%
filter(
!is.na(filings_ma3),
!is.na(spatial_lag_filings),
!is.na(delinquent_prop_count),
!is.na(pct_renter),
!is.na(poverty_rate),
!is.na(severe_rent_burden),
!is.na(pct_single_mother),
!is.na(racial_majority),
!is.na(moratorium_active)
)
# Apply the cap only to training target variable to make it stable.
# Create training set where filings are capped at 99.75 percentile = 20 filings.
df_train <- df_filtered %>%
filter(date <= train_end_date) %>%
mutate(filings_count_capped = pmin(filings_count, cap_threshold))
# Create testing set where target is including spikes for real-world.
df_test <- df_filtered %>%
filter(date >= test_start_date)
# Display split statistics.
cat("TRAIN/TEST SPLIT SUMMARY\n")
cat(sprintf("Training Period: %s to %s\n", min(df_train$date), max(df_train$date)))
cat(sprintf("Training Observations: %s\n", comma(nrow(df_train))))
cat(sprintf("Training Months: %d\n", n_distinct(df_train$date)))
cat(sprintf("Test Period: %s to %s\n", min(df_test$date), max(df_test$date)))
cat(sprintf("Test Observations: %s\n", comma(nrow(df_test))))
cat(sprintf("Test Months: %d\n", n_distinct(df_test$date)))
# Top are the numbers, below are the frequencies, alternating.
# Train set table.
print(table(df_train$filings_count))
# Test set table.
print(table(df_test$filings_count))
# Adjust so that the extreme outliers are not included. Cap lagged momentum signal to reduce distortion from rare mass-eviction spikes, which is 99.8 percentile.
df_train <- df_train %>% mutate(filings_ma3 = pmin(filings_ma3, 20))
df_test <- df_test %>% mutate(filings_ma3 = pmin(filings_ma3, 20))
# Adjust so that the extreme outliers are not included. Cap lagged momentum signal to reduce distortion from rare mass-eviction spikes, which is 99.8 percentile.
df_train <- df_train %>% mutate(filings_ma3 = pmin(filings_ma3, 20))
df_test <- df_test %>% mutate(filings_ma3 = pmin(filings_ma3, 20))
# Compare filing distributions between train and test sets.
train_test_comparison <- bind_rows(
df_train %>% mutate(set = "training"),
df_test %>% mutate(set = "test")
) %>%
group_by(set) %>%
summarize(
n_obs = n(),
mean_filings = mean(filings_count, na.rm = TRUE),
median_filings = median(filings_count, na.rm = TRUE),
sd_filings = sd(filings_count, na.rm = TRUE),
zero_pct = sum(filings_count == 0) / n() * 100,
.groups = "drop"
)
train_test_comparison %>%
kable(digits = 2, caption = "Filing Distribution Comparison: Training vs Test Sets") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Fit baseline Negative Binomial model with core time, policy, and monthly predictors.
model_nb_1 <- glm.nb(filings_count_capped ~ filings_ma3 + moratorium_active + factor(month_num) + is_extreme_spike,
data = df_train)
# Display 1st model summary.
cat("BASELINE NEGATIVE BINOMIAL MODEL SUMMARY\n")
summary(model_nb_1)
model_nb_2 <- glm.nb(filings_count_capped ~
filings_ma3 + moratorium_active + factor(month_num) + is_extreme_spike +
# Added spatial lag and delinquent properties because they're more actionable.
spatial_lag_filings + delinquent_prop_count,
data = df_train)
# Display 2nd model summary.
cat("2ND NEGATIVE BINOMIAL MODEL SUMMARY\n")
summary(model_nb_2)
model_nb_3 <- glm.nb(filings_count_capped ~
# 1st model.
filings_ma3 + moratorium_active + factor(month_num) + is_extreme_spike +
# 2nd model.
spatial_lag_filings + delinquent_prop_count +
# Add demographics.
pct_renter + severe_rent_burden + poverty_rate + pct_single_mother +
factor(racial_majority),
data = df_train)
# Display 3rd model summary.
cat("3RD NEGATIVE BINOMIAL MODEL SUMMARY\n")
summary(model_nb_3)
model_nb_4 <- glm.nb(filings_count_capped ~
# 1st model.
filings_ma3 + moratorium_active + factor(month_num) + is_extreme_spike +
# 2nd model.
spatial_lag_filings + delinquent_prop_count +
# 3rd model.
pct_renter + severe_rent_burden + poverty_rate + pct_single_mother +
factor(racial_majority) +
# Add interactions.
factor(racial_majority) * moratorium_active,
data = df_train)
cat("FULL NEGATIVE BINOMIAL MODEL SUMMARY\n")
summary(model_nb_4)
# Compare model fit statistics.
model_comparison <- data.frame(
model = c("Baseline", "+ Tax and Spatial", "+ ACS", "+ Interactions"),
aic = c(AIC(model_nb_1), AIC(model_nb_2), AIC(model_nb_3), AIC(model_nb_4)),
theta = c(model_nb_1$theta, model_nb_2$theta, model_nb_3$theta, model_nb_4$theta),
loglik = c(logLik(model_nb_1), logLik(model_nb_2), logLik(model_nb_3), logLik(model_nb_4))
) %>%
mutate(
aic_change = aic - min(aic),
rank = rank(aic)
)
model_comparison %>%
kable(digits = 2, caption = "Model Comparison Statistics (Lower AIC = Better Fit)") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# TRAINING SET PREDICTIONS.
# Model 1: Baseline.
train_pred_1 <- df_train %>%
mutate(
predicted = predict(model_nb_1, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# Model 2: + Tax/Spatial.
train_pred_2 <- df_train %>%
mutate(
predicted = predict(model_nb_2, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# Model 3: + ACS.
train_pred_3 <- df_train %>%
mutate(
predicted = predict(model_nb_3, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# Model 4: + Interactions.
train_pred_4 <- df_train %>%
mutate(
predicted = predict(model_nb_4, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# TEST SET PREDICTIONS.
# Model 1: Baseline.
test_pred_1 <- df_test %>%
mutate(
predicted = predict(model_nb_1, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
# Model 2: + Tax/Spatial.
test_pred_2 <- df_test %>%
mutate(
predicted = predict(model_nb_2, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
# Model 3: + ACS.
test_pred_3 <- df_test %>%
mutate(
predicted = predict(model_nb_3, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
# Model 4: + Interactions.
test_pred_4 <- df_test %>%
mutate(
predicted = predict(model_nb_4, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
cat(sprintf("Training Set: %s observations\n", comma(nrow(train_pred_1))))
cat(sprintf("Test Set: %s observations\n", comma(nrow(test_pred_1))))
# Calculate training metrics for each model explicitly.
train_metrics_1 <- train_pred_1 %>%
summarize(
model = "Baseline",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
train_metrics_2 <- train_pred_2 %>%
summarize(
model = "+ Tax/Spatial",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
train_metrics_3 <- train_pred_3 %>%
summarize(
model = "+ ACS",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
train_metrics_4 <- train_pred_4 %>%
summarize(
model = "+ Interactions",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
# Combine training metrics.
train_metrics <- bind_rows(train_metrics_1, train_metrics_2, train_metrics_3, train_metrics_4)
# Display training performance.
train_metrics %>%
kable(digits = 3, caption = "Training Set Performance Across Models") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Compare model fit statistics.
model_comparison <- data.frame(
model = c("Baseline", "+ Tax and Spatial", "+ ACS", "+ Interactions"),
aic = c(AIC(model_nb_1), AIC(model_nb_2), AIC(model_nb_3), AIC(model_nb_4)),
theta = c(model_nb_1$theta, model_nb_2$theta, model_nb_3$theta, model_nb_4$theta),
loglik = c(logLik(model_nb_1), logLik(model_nb_2), logLik(model_nb_3), logLik(model_nb_4))
) %>%
mutate(
aic_change = aic - min(aic),
rank = rank(aic)
)
model_comparison %>%
kable(digits = 2, caption = "Model Comparison Statistics (Lower AIC = Better Fit)") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# TRAINING SET PREDICTIONS.
# Model 1: Baseline.
train_pred_1 <- df_train %>%
mutate(
predicted = predict(model_nb_1, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# Model 2: + Tax/Spatial.
train_pred_2 <- df_train %>%
mutate(
predicted = predict(model_nb_2, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# Model 3: + ACS.
train_pred_3 <- df_train %>%
mutate(
predicted = predict(model_nb_3, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# Model 4: + Interactions.
train_pred_4 <- df_train %>%
mutate(
predicted = predict(model_nb_4, newdata = ., type = "response"),
residual = filings_count_capped - predicted,
abs_error = abs(residual)
)
# TEST SET PREDICTIONS.
# Model 1: Baseline.
test_pred_1 <- df_test %>%
mutate(
predicted = predict(model_nb_1, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
# Model 2: + Tax/Spatial.
test_pred_2 <- df_test %>%
mutate(
predicted = predict(model_nb_2, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
# Model 3: + ACS.
test_pred_3 <- df_test %>%
mutate(
predicted = predict(model_nb_3, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
# Model 4: + Interactions.
test_pred_4 <- df_test %>%
mutate(
predicted = predict(model_nb_4, newdata = ., type = "response"),
residual = filings_count - predicted,
abs_error = abs(residual)
)
cat(sprintf("Training Set: %s observations\n", comma(nrow(train_pred_1))))
cat(sprintf("Test Set: %s observations\n", comma(nrow(test_pred_1))))
# Calculate training metrics for each model explicitly.
train_metrics_1 <- train_pred_1 %>%
summarize(
model = "Baseline",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
train_metrics_2 <- train_pred_2 %>%
summarize(
model = "+ Tax/Spatial",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
train_metrics_3 <- train_pred_3 %>%
summarize(
model = "+ ACS",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
train_metrics_4 <- train_pred_4 %>%
summarize(
model = "+ Interactions",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count_capped, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count_capped, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count_capped, na.rm = TRUE)
)
# Combine training metrics.
train_metrics <- bind_rows(train_metrics_1, train_metrics_2, train_metrics_3, train_metrics_4)
# Display training performance.
train_metrics %>%
kable(digits = 3, caption = "Training Set Performance Across Models") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Calculate test metrics for each model explicitly.
test_metrics_1 <- test_pred_1 %>%
summarize(
model = "Baseline",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count, na.rm = TRUE)
)
test_metrics_2 <- test_pred_2 %>%
summarize(
model = "+ Tax/Spatial",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count, na.rm = TRUE)
)
test_metrics_3 <- test_pred_3 %>%
summarize(
model = "+ ACS",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count, na.rm = TRUE)
)
test_metrics_4 <- test_pred_4 %>%
summarize(
model = "+ Interactions",
n = n(),
MAE = mean(abs_error, na.rm = TRUE),
RMSE = sqrt(mean(residual^2, na.rm = TRUE)),
mean_observed = mean(filings_count, na.rm = TRUE),
mean_predicted = mean(predicted, na.rm = TRUE),
correlation = cor(filings_count, predicted, use = "complete.obs"),
bias = mean(predicted - filings_count, na.rm = TRUE)
)
# Combine test metrics.
test_metrics <- bind_rows(test_metrics_1, test_metrics_2, test_metrics_3, test_metrics_4)
# Display test performance.
test_metrics %>%
kable(digits = 3, caption = "Test Set Performance Across Models") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
